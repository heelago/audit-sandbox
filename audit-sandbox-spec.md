# AI Audit Sandbox — System Architecture & Technical Specification

**Version 0.1 — Draft for Development & Academic Documentation**

---

## 1. System Overview

The AI Audit Sandbox is an assessment platform for higher education that inverts the conventional relationship between AI and academic integrity. Rather than attempting to detect or prevent AI use, the system positions AI-generated text as the object of critical analysis. The student's assignment is not to write — it is to audit.

The system operates on a six-layer pipeline:

```
┌─────────────────────────────────────────────────────────────────┐
│  LAYER 1: TEXT GENERATION                                       │
│  Standardized prompt → AI model → N unique outputs              │
├─────────────────────────────────────────────────────────────────┤
│  LAYER 2: AUTOMATED BASELINE ANALYSIS                           │
│  Series of dedicated analytical passes → Hidden rubric          │
├─────────────────────────────────────────────────────────────────┤
│  LAYER 3: PROFESSOR CALIBRATION                                 │
│  Sample review → Human-flagged issues → Additional auto passes  │
├─────────────────────────────────────────────────────────────────┤
│  LAYER 4: STUDENT AUDIT WORKSPACE                               │
│  Annotation, evidence attachment, reflection                    │
├─────────────────────────────────────────────────────────────────┤
│  LAYER 5: SCORING ENGINE                                        │
│  Student annotations ↔ Hidden rubric comparison → 3-tier score  │
├─────────────────────────────────────────────────────────────────┤
│  LAYER 6: REPORTING & REVIEW                                    │
│  Student report + Instructor dashboard + Professor override     │
└─────────────────────────────────────────────────────────────────┘
```

The central design principle is that the hidden rubric — generated before students ever see the text — creates a standardized, fair, and largely automatable assessment framework, while the "beyond rubric" scoring tier preserves space for genuine disciplinary expertise to be recognized and rewarded.

---

## 2. Layer 1: Text Generation

### 2.1 Purpose

Generate a unique AI-authored text for each student from a shared, instructor-defined prompt. The texts serve as standardized examination objects — comparable in difficulty but not identical, reflecting realistic conditions of AI use.

### 2.2 Process

```
Instructor defines prompt
        │
        ▼
System stores prompt (immutable for this assignment cycle)
        │
        ▼
On assignment release: generate N texts (one per enrolled student)
        │
        ▼
All texts generated by same model, same prompt, same parameters
        │
        ▼
Each text stored with metadata:
  - generation_id
  - model_version
  - timestamp
  - token_count
  - raw_prompt (exact string sent to API)
```

### 2.3 Prompt Design

The instructor provides the assignment prompt through a structured interface with the following fields:

- **Topic & scope**: The subject matter and boundaries (e.g., "social class and educational attainment in sociology")
- **Requirements**: Structural expectations (word count, number of sources, sections expected)
- **Register**: Formal/informal, field-specific conventions
- **Known pitfalls** (optional): The instructor can flag areas where AI commonly fails in this domain, which informs Layer 2 passes

The prompt is stored as read-only for the assignment cycle. Future versions may explore student prompt customization, but the initial design prioritizes fairness through standardization.

### 2.4 Generation Parameters

- **Model**: Single model per assignment cycle (e.g., Claude Sonnet). Model version is logged.
- **Temperature**: Fixed at a moderate value (e.g., 0.7) to produce variation without instability
- **No system prompt manipulation**: The generation prompt is the instructor's prompt alone, with no hidden instructions to introduce or suppress errors. The AI's natural error patterns *are* the assessment material.
- **Output validation**: Basic checks for completion (minimum word count reached, text is coherent, references section present if requested). Regenerate on failure.

### 2.5 Fairness Consideration: Difficulty Variance

Because identical prompts produce variable outputs, some texts will contain more auditable issues than others. This is addressed in two ways:

1. **Layer 2 quantifies the difficulty** of each text, producing a difficulty score that the scoring engine uses for normalization
2. **Scoring is process-based, not checklist-based**: A student who thoroughly engages with a "cleaner" text and finds fewer issues but verifies claims rigorously can score equivalently to one who catches many errors in a more problematic text

### 2.6 Prompt Design Strategy: Terrain Selection Over Error Seeding

A natural question arises: should the system deliberately introduce inaccuracies into the generation prompt to guarantee every text contains identifiable errors?

**The answer is no.** The system does not seed, plant, or manipulate errors in generated texts. This is a deliberate design principle, not an oversight.

**Rationale — authenticity over control:**

1. **AI's natural failure modes are the assessment material.** The pedagogical goal is to teach students to recognize how AI actually fails — subtle misattributions, flattened disciplinary nuances, unverifiable statistics presented with false confidence, Western-centric framings treated as universal. Seeded errors are clean and identifiable; real AI failures are messy and disciplinary. Students need practice with the latter.

2. **Trust and defensibility.** If students discover errors were planted, the exercise is reframed as a trick rather than genuine critical engagement. The current design's honesty — "this is what AI actually produced, now show me what you see" — is pedagogically cleaner and institutionally defensible.

3. **Seeded errors teach the wrong skill.** Students who learn to find planted mistakes are learning to find *the instructor's* errors. Students who learn to find natural AI failures are developing a transferable professional competency.

**What instructors CAN do — terrain selection:**

Instead of manipulating the text, instructors design prompts that steer toward topics where AI is known to struggle in discipline-relevant ways. This is good assignment design, not manipulation. Examples:

- **Contested theoretical interpretations**: "Discuss the debate between Bourdieu's cultural reproduction theory and Willis's resistance theory" — AI tends to flatten these into false synthesis
- **Cross-cultural comparisons**: "Compare educational stratification in Scandinavian welfare states and East Asian meritocratic systems" — AI tends toward Western-centric default framings
- **Methodological nuance**: "Evaluate the evidence for school tracking's causal effect on inequality" — AI struggles with the distinction between correlation and causal inference
- **Recent developments**: "Discuss post-COVID changes in educational inequality patterns" — AI may hallucinate or conflate pre- and post-pandemic data
- **Intersectional analysis**: "Examine how class, race, and gender interact in educational outcomes" — AI tends to treat these as parallel rather than intersecting

The instructor's "Known pitfalls" field in the prompt interface (Section 2.3) accommodates this strategy: the instructor notes where they expect AI to struggle, which both sharpens the generation prompt and informs Layer 2's analytical passes.

This approach preserves the system's epistemic integrity while ensuring that generated texts contain enough auditble material for meaningful assessment. The automated baseline (Layer 2) confirms this: if a generated text scores too clean (difficulty rating below a configurable threshold), the system flags it and offers the instructor the option to regenerate.

---

## 3. Layer 2: Automated Baseline Analysis

### 3.1 Purpose

Before any student sees their text, the system runs a series of dedicated analytical passes to build a hidden rubric — a structured inventory of known issues, verification-worthy claims, and quality assessments for each generated text.

### 3.2 Analysis Passes

Each pass is a separate API call with a specialized prompt. Passes run sequentially; each can reference findings from previous passes.

#### Pass 1: Factual Accuracy Scan

```
Input:  Generated text
Prompt: Examine every factual claim in this text. For each claim, assess:
        - Is it verifiable?
        - Is it accurate based on your knowledge?
        - Confidence level (high/medium/low)
        - If inaccurate: what is the correct information?
        - If unverifiable: what would verification require?

Output: Structured JSON array of claim objects:
        {
          claim_text: "students in the bottom income quartile are 38% less likely...",
          location: { start: 842, end: 921 },
          type: "statistical",
          accuracy: "unverifiable",
          confidence: "low",
          note: "Specific statistic not traceable to a named 2019 OECD report.
                 OECD Education at a Glance reports exist but this figure
                 may be fabricated or conflated.",
          verification_method: "Check OECD Education at a Glance 2019,
                               PISA reports, Education Indicators in Focus"
        }
```

#### Pass 2: Source & Citation Verification

```
Input:  Generated text + Pass 1 findings
Prompt: Examine every citation and source reference. For each:
        - Does this publication exist?
        - Is the publication date correct?
        - Does the author actually argue what is attributed to them?
        - Are there misattributions or oversimplifications?
        - Are any "citations" potentially hallucinated?

Output: Structured JSON array of citation objects:
        {
          citation: "Bourdieu & Passeron, 1977",
          work_title: "Reproduction in Education, Society and Culture",
          exists: true,
          date_correct: true,
          attribution_accurate: "partial",
          note: "The text attributes 'linguistic codes' to Bourdieu & Passeron,
                 but the concept of 'elaborated and restricted codes' originates
                 with Bernstein (1971), not Bourdieu. Bourdieu's framework
                 uses 'cultural capital' and 'habitus' rather than 'codes'."
        }
```

#### Pass 3: Structural & Argumentative Analysis

```
Input:  Generated text + Pass 1-2 findings
Prompt: Analyze the text's argumentative structure:
        - Is the thesis clearly stated and supported?
        - Are there logical gaps or non-sequiturs?
        - Are causal claims properly qualified?
        - Is the conclusion supported by the body?
        - Are counterarguments addressed?

Output: Structured JSON of structural observations
```

#### Pass 4: Disciplinary Depth & Framing Analysis

```
Input:  Generated text + Pass 1-3 findings + field context from instructor prompt
Prompt: As a specialist in [field from instructor prompt], evaluate:
        - Are key debates in the field represented fairly?
        - Is the framing culturally situated or presented as universal?
        - Are important theoretical perspectives missing?
        - Does the text oversimplify mechanisms that specialists
          would recognize as more complex?
        - Are there methodological assumptions that go unexamined?

Output: Structured JSON of disciplinary observations
```

#### Pass 5: Gap Analysis

```
Input:  Generated text + Pass 1-4 findings + instructor requirements
Prompt: Given the assignment requirements, identify:
        - Required elements that are missing
        - Perspectives or populations not discussed
        - Relevant recent developments not mentioned
        - Policy implications not explored (if required)
        - Methodological considerations absent

Output: Structured JSON of identified gaps
```

#### Pass 6: Composite Scoring & Difficulty Rating

```
Input:  All previous pass outputs
Prompt: Synthesize all findings into:
        - Total number of identified issues by severity
          (critical / moderate / minor)
        - Difficulty rating (1-10): how challenging is this text to audit?
        - Priority issues: which findings are most important
          for a student to catch?
        - Expected baseline: what percentage of issues should
          a competent student reasonably find?

Output: Composite rubric JSON with difficulty metadata
```

### 3.3 Output: The Hidden Rubric

Each generated text now has an associated rubric object:

```json
{
  "text_id": "gen_2025_soc101_042",
  "difficulty_rating": 6.8,
  "total_issues": 14,
  "severity_distribution": {
    "critical": 3,
    "moderate": 7,
    "minor": 4
  },
  "issues": [
    {
      "id": "issue_001",
      "pass_source": "factual_accuracy",
      "severity": "moderate",
      "location": { "start": 842, "end": 921 },
      "category": "unverifiable_statistic",
      "description": "38% figure not traceable to specific OECD source",
      "ideal_student_response": "Flag as requiring verification,
                                 note that the specific report is not named,
                                 attempt to locate the actual OECD data"
    }
  ],
  "raw_text_quality_score": 72,
  "expected_student_detection_rate": 0.55
}
```

### 3.4 Design Principle: Deliberate Incompleteness

The automated passes are designed to be thorough but not exhaustive. The system deliberately stops short of identifying every possible critique for two reasons:

1. **Preserving space for "beyond rubric" findings**: Students who bring genuine disciplinary knowledge should be able to catch things the system didn't. This is where the deepest learning happens, and it must be rewarded.
2. **Avoiding over-specification**: If the rubric is too comprehensive, grading becomes a mechanical matching exercise rather than an assessment of critical thinking.

The expected detection rate (e.g., 0.55) represents the proportion of rubric items a competent student should find. This is used for normalization, not as a ceiling.

---

## 4. Layer 3: Professor Calibration

### 4.1 Purpose

The automated passes produce a strong baseline, but domain expertise catches what automated analysis misses. The professor calibration loop adds human judgment to the rubric without requiring the professor to review every text.

### 4.2 Process

```
System selects 2-3 sample texts for professor review
  (selection criteria: one high-difficulty, one medium, one low)
        │
        ▼
Professor reviews samples in calibration interface:
  - Sees generated text alongside automated findings
  - Can confirm, modify, or dismiss automated findings
  - Can flag NEW issues the automated passes missed
        │
        ▼
Professor-flagged issues are converted to additional prompts:
  "Check all generated texts for: [professor's observation]"
        │
        ▼
System runs additional targeted passes on ALL texts
        │
        ▼
Updated rubrics are generated
        │
        ▼
Professor reviews updated difficulty distribution
  - Can adjust expected detection rates
  - Can weight certain issue categories higher
  - Approves final rubric configuration
```

### 4.3 Calibration Interface

The professor sees a split view:

**Left panel**: The generated text with automated findings highlighted inline (color-coded by severity and type)

**Right panel**: A structured list of findings with:
- Confirm / Dismiss / Modify buttons per finding
- Free-text field to add observations
- "Flag for all texts" checkbox — when checked, the observation becomes a new automated pass

### 4.4 Converting Professor Observations to Automated Passes

When a professor flags something like: *"The text misrepresents Bourdieu's position on linguistic capital — this is actually Bernstein's concept"*, the system generates a targeted prompt:

```
Input:  Each generated text
Prompt: A domain expert noted that AI-generated texts in this
        assignment frequently misattribute the concept of
        "linguistic codes" (elaborated/restricted) to Bourdieu,
        when this concept originates with Bernstein (1971).

        Examine this text for:
        1. Any misattribution of Bernstein's concepts to Bourdieu
        2. Any conflation of "cultural capital" with "linguistic codes"
        3. Similar theoretical misattributions across other authors

Output: Findings specific to this observation
```

This process is iterative: the professor can review results of the additional passes and flag further issues if needed. In practice, 1-2 calibration rounds should suffice for most assignments.

### 4.5 Difficulty Normalization After Calibration

After calibration, the system recalculates difficulty ratings for all texts. The professor can then review the distribution:

```
Difficulty Distribution (N=30 students):
  Rating 3-4 (easier):   ████ 4 texts
  Rating 5-6 (moderate):  ████████████████ 16 texts
  Rating 7-8 (harder):    ████████ 8 texts
  Rating 9+ (very hard):  ██ 2 texts

Mean: 6.1 | Std Dev: 1.4
```

If the distribution is too skewed, the professor can regenerate outlier texts or adjust scoring weights.

---

## 5. Layer 4: Student Audit Workspace

### 5.1 Purpose

Provide students with a structured environment to critically examine their AI-generated text, annotate it, attach evidence, and produce a documented audit trail. The workspace is designed to create productive friction — every interaction generates gradable data.

### 5.2 Student-Facing Flow

```
Student opens assignment
        │
        ▼
Landing page:
  - Assignment title and context
  - Read-only prompt display
  - Notice: "This text was generated by AI from a standardized prompt.
    All students receive the same prompt. Outputs may vary.
    Your audit is your assignment."
        │
        ▼
Workspace loads with:
  - Full AI-generated text (main panel)
  - Annotation sidebar
  - Guide / scoring rubric reference
        │
        ▼
Student workflow:
  1. Read through text
  2. Select passages → tag with annotation type
  3. Write reflection / analysis for each annotation
  4. Attach evidence (sources, AI conversations, process notes)
  5. Review coverage (% of text examined)
  6. Generate and submit audit report
```

### 5.3 Annotation Types

Six annotation categories, designed to map onto gradable dimensions:

| Type | Label (EN) | Label (HE) | What it captures |
|------|-----------|------------|-----------------|
| error | Inaccuracy Found | אי-דיוק | Factual, conceptual, or logical errors |
| verified | Verified Accurate | אומת כמדויק | Claims independently checked and confirmed |
| alternative | Better Alternative | חלופה עדיפה | Stronger approaches, framings, or sources |
| gap | Missing Element | רכיב חסר | Important content the AI omitted |
| nuance | Disciplinary Nuance | ניואנס דיסציפלינרי | Field-specific insights AI missed |
| accepted | Accepted — With Reason | אושר — עם נימוק | Deliberately kept, with justification |

### 5.4 Evidence Types

| Type | Purpose |
|------|---------|
| AI Conversation | Document exchanges with AI tools used during verification |
| Source / Verification | URLs, article references, database records used to check claims |
| Process Note | Reasoning documentation — decision rationale, observations |

### 5.5 Guided Prompts

Each annotation type triggers context-specific reflection prompts. These serve as scaffolding — they lower the barrier to meaningful reflection without dictating content.

Example for "Inaccuracy Found":
- What specifically is wrong?
- How did you discover the error?
- What is the correct information?

Example for "Better Alternative":
- What would you propose instead?
- Why is your alternative stronger?
- Would the AI have known this?

### 5.6 Data Captured

Every student interaction generates structured data:

```json
{
  "annotation_id": "ann_001",
  "student_id": "stu_042",
  "text_id": "gen_2025_soc101_042",
  "type": "error",
  "location": { "start": 842, "end": 921 },
  "selected_text": "students in the bottom income quartile are 38%...",
  "note": "I couldn't find this exact statistic in any OECD report...",
  "evidence": [
    {
      "type": "source",
      "content": "OECD Education at a Glance 2019 — Table A4.1 shows...",
      "timestamp": "2025-02-22T14:32:00Z"
    }
  ],
  "guided_prompts_used": ["How did you discover the error?"],
  "created_at": "2025-02-22T14:28:00Z",
  "last_edited": "2025-02-22T14:35:00Z",
  "time_spent_seconds": 420
}
```

---

## 6. Layer 5: Scoring Engine

### 6.1 Purpose

Compare student annotations against the hidden rubric to produce a structured, three-tier assessment. The scoring engine handles the mechanical comparison; the professor makes final judgments.

### 6.2 Three-Tier Scoring Model

```
┌─────────────────────────────────────────────────────────┐
│  TIER 1: RUBRIC MATCHES                                 │
│  Student found issues that the hidden rubric identified  │
│  → Baseline competency                                   │
├─────────────────────────────────────────────────────────┤
│  TIER 2: RUBRIC MISSES                                  │
│  Issues in the hidden rubric that the student missed     │
│  → Deductions or development indicators                  │
├─────────────────────────────────────────────────────────┤
│  TIER 3: BEYOND RUBRIC                                  │
│  Student found issues NOT in the hidden rubric           │
│  → Bonus / excellence indicators                         │
└─────────────────────────────────────────────────────────┘
```

### 6.3 Matching Algorithm

Matching student annotations to rubric items requires fuzzy alignment, since students won't select exactly the same text spans or use the same terminology as the automated passes.

```
For each student annotation:
  1. LOCATION OVERLAP: Calculate character overlap between
     student selection and each rubric item's location.
     Threshold: >30% overlap = potential match.

  2. SEMANTIC SIMILARITY: Compare student's note content
     against rubric item description using embedding similarity.
     Threshold: >0.65 cosine similarity = potential match.

  3. TYPE ALIGNMENT: Check if student's annotation type
     is compatible with rubric item's category.
     (e.g., student tags "error" ↔ rubric item is "factual_inaccuracy")

  4. MATCH CONFIDENCE:
     - High: location overlap >60% AND semantic similarity >0.75
     - Medium: location overlap >30% OR semantic similarity >0.65
     - Low: only one signal, below thresholds

  5. MATCH QUALITY: Even when matched, assess the quality
     of the student's analysis:
     - Did they correctly identify WHAT is wrong?
     - Did they provide the correct information?
     - Did they attach supporting evidence?
     - Is their reasoning sound?
```

### 6.4 Scoring Rubric

#### Tier 1 — Rubric Matches (up to 60% of total grade)

| Quality | Points | Criteria |
|---------|--------|----------|
| Strong match | Full points | Correct identification + accurate analysis + evidence |
| Partial match | 60-80% | Correct identification but incomplete analysis or missing evidence |
| Weak match | 30-50% | Right area flagged but wrong diagnosis or no reasoning |

Points are weighted by issue severity:
- Critical issues: 3x weight
- Moderate issues: 2x weight
- Minor issues: 1x weight

#### Tier 2 — Rubric Misses (up to -20% deduction)

| Severity of missed issue | Deduction |
|--------------------------|-----------|
| Critical issue missed | Significant deduction |
| Moderate issue missed | Moderate deduction |
| Minor issue missed | Minimal or no deduction |
| Issue in unannotated section | Flagged but weighted by text coverage |

Deductions are normalized by difficulty rating. A student whose text had a difficulty rating of 8.5 is expected to miss more issues than one with a rating of 4.2.

The "Accepted — With Reason" annotation type interacts with Tier 2: if a student marked a passage containing a rubric issue as "Accepted" but provided weak or no justification, the deduction is applied. If they accepted it with strong reasoning that acknowledges the limitation, the deduction is reduced or waived.

#### Tier 3 — Beyond Rubric (up to 20% bonus)

| Quality | Points | Criteria |
|---------|--------|----------|
| Validated novel finding | Full bonus | Genuine issue not in rubric + evidence + sound reasoning |
| Plausible novel finding | Partial bonus | Reasonable critique but unverified or debatable |
| False positive | No points | Student flagged something that isn't actually an issue |

Beyond-rubric findings are flagged for professor review. The system can pre-assess them using an additional API call:

```
Input:  Student annotation + original text + existing rubric
Prompt: A student flagged the following issue that was NOT
        identified in the automated analysis:

        [student annotation + note + evidence]

        Assess: Is this a legitimate finding? Is the student's
        analysis correct? Would a domain expert agree?

Output: { legitimate: true/false, quality: "high"/"medium"/"low",
          reasoning: "..." }
```

### 6.5 Coverage Scoring

Text coverage (percentage of text that received at least one annotation) is a scoring factor:

| Coverage | Interpretation |
|----------|---------------|
| >80% | Thorough examination |
| 60-80% | Adequate examination |
| 40-60% | Partial examination — flag for instructor |
| <40% | Insufficient engagement — significant concern |

Coverage scoring is adjusted for text length and the distribution of known issues. If all rubric items are concentrated in the first three paragraphs, a student who covered 50% of the text but found all issues scores differently than one who annotated 50% but missed the dense sections.

### 6.6 Composite Score Calculation

```
Raw Score = Tier1_Points - Tier2_Deductions + Tier3_Bonus

Normalized Score = Raw Score × Difficulty_Adjustment_Factor

Difficulty_Adjustment_Factor =
  mean_difficulty / student_text_difficulty
  (capped at 0.85 to 1.15 to prevent extreme adjustments)

Final Score = min(100, max(0, Normalized Score))
```

---

## 7. Layer 6: Reporting & Review

### 7.1 Student-Facing Report

Generated automatically upon submission. Contains:

**Summary statistics**: Annotation count, text coverage, evidence count, reflection depth

**Annotation breakdown**: By type, with distribution visualization

**Scoring indicators** (non-numeric): Students see qualitative feedback rather than raw scores before professor review:
- "You identified 8 issues and verified 5 claims — strong engagement"
- "Consider examining the concluding paragraph more carefully"
- "Your alternative proposals showed strong disciplinary knowledge"

**Full annotation log**: All annotations with evidence, organized by text position

The student report deliberately does NOT reveal:
- The hidden rubric
- Which of their findings matched rubric items
- Specific issues they missed
- Their numeric score (until professor releases grades)

### 7.2 Instructor Dashboard

The professor sees a class-level view with drill-down capability:

#### Class Overview

```
Assignment: SOC 101 — Social Class & Educational Attainment
Students: 30 | Submitted: 28 | Pending: 2

Score Distribution:
  90-100: ████ 4
  80-89:  ████████████ 11
  70-79:  ████████ 8
  60-69:  ███ 3
  <60:    ██ 2

Mean: 78.4 | Median: 80.1 | Std Dev: 11.2

Most Frequently Caught Issues:
  1. OECD statistic unverifiable (24/28 students found this)
  2. Bernstein/Bourdieu misattribution (18/28)
  3. Finland oversimplification (15/28)

Most Frequently Missed Issues:
  1. Western-centric framing of "tracking" (only 4/28 caught)
  2. Absence of intersectional analysis (only 6/28)
  3. Lareau's methodology limitations (only 3/28)

Notable Beyond-Rubric Findings:
  - Student 014: Identified missing reference to Willis (1977)
    "Learning to Labour" — confirmed legitimate [REVIEW]
  - Student 023: Argued conditional cash transfers paragraph
    conflates enrollment with attendance — confirmed [REVIEW]
```

#### Per-Student View

```
Student: [Name] | Text Difficulty: 6.8 | Raw Score: 82

TIER 1 — RUBRIC MATCHES (48/60)
  ✓ OECD statistic flagged as unverifiable [STRONG MATCH]
    Student note: "Could not locate this specific figure..."
    Evidence: Link to OECD EaG 2019 Table A4.1
    Quality: ████████░░ 8/10

  ✓ Bernstein/Bourdieu confusion identified [PARTIAL MATCH]
    Student note: "Bourdieu used 'cultural capital' not 'codes'"
    Evidence: None attached
    Quality: ██████░░░░ 6/10 — correct identification but
             incomplete (didn't name Bernstein as source)

TIER 2 — RUBRIC MISSES (-8)
  ✗ Western-centric framing of tracking systems [MISSED]
    Rubric note: Text presents German/Dutch tracking as
    the primary model, ignoring non-Western sorting mechanisms
  ✗ No engagement with reference list accuracy [MISSED]
    Three of five references were left unexamined

TIER 3 — BEYOND RUBRIC (+6)
  ★ Flagged absence of race/ethnicity as intersecting factor
    Quality: High — well-argued with reference to Crenshaw
    AI assessment: Legitimate, not in rubric [CONFIRM / REJECT]

PROFESSOR ACTIONS:
  [Confirm all AI assessments]
  [Adjust individual scores]
  [Add comments for student]
  [Release grade]
```

### 7.3 Professor Override Capabilities

The professor can:

1. **Adjust any tier score** with a written rationale (logged for audit)
2. **Confirm or reject** beyond-rubric assessments
3. **Add qualitative comments** visible to the student
4. **Flag exceptional work** for class discussion (anonymized)
5. **Batch-release grades** with optional individual holds
6. **Export data** for research purposes (anonymized)

### 7.4 Aggregate Analytics (Multi-Assignment)

Over time, the system collects data on:

- **Common AI failure patterns** by discipline and topic
- **Student development trajectories**: Are students catching more issues over the semester?
- **Rubric calibration accuracy**: How often do professor overrides change scores significantly?
- **Beyond-rubric finding rates**: Are students developing expertise beyond what the system detects?
- **Annotation type distributions**: Are students overusing "accepted" relative to other types?

---

## 8. Technical Infrastructure

### 8.1 Database Schema (Simplified)

```
assignments
  ├── id
  ├── course_id
  ├── title
  ├── prompt_text
  ├── model_version
  ├── generation_parameters
  ├── status (draft / generating / calibrating / active / grading / closed)
  └── created_at

generated_texts
  ├── id
  ├── assignment_id
  ├── student_id
  ├── text_content
  ├── word_count
  ├── generation_metadata
  └── created_at

rubric_items
  ├── id
  ├── text_id
  ├── pass_source (factual / citation / structural / disciplinary / gap / professor)
  ├── severity (critical / moderate / minor)
  ├── category
  ├── location_start
  ├── location_end
  ├── description
  ├── ideal_response
  └── created_at

text_quality_scores
  ├── text_id
  ├── difficulty_rating
  ├── total_issues
  ├── severity_distribution (JSON)
  ├── expected_detection_rate
  └── raw_quality_score

calibration_logs
  ├── id
  ├── assignment_id
  ├── professor_id
  ├── action (confirm / dismiss / modify / flag_new)
  ├── rubric_item_id (nullable)
  ├── notes
  └── created_at

annotations
  ├── id
  ├── student_id
  ├── text_id
  ├── type (error / verified / alternative / gap / nuance / accepted)
  ├── location_start
  ├── location_end
  ├── selected_text
  ├── note
  ├── created_at
  ├── last_edited
  └── time_spent_seconds

evidence
  ├── id
  ├── annotation_id
  ├── type (conversation / source / note)
  ├── content
  └── created_at

scores
  ├── id
  ├── student_id
  ├── text_id
  ├── tier1_raw
  ├── tier2_deductions
  ├── tier3_bonus
  ├── coverage_score
  ├── difficulty_adjustment
  ├── composite_raw
  ├── normalized_final
  ├── professor_adjusted (boolean)
  ├── professor_override_score (nullable)
  ├── professor_notes
  └── released_at

rubric_matches
  ├── annotation_id
  ├── rubric_item_id
  ├── match_confidence (high / medium / low)
  ├── match_quality_score
  └── auto_assessed_at

beyond_rubric_findings
  ├── annotation_id
  ├── ai_legitimacy_assessment
  ├── ai_quality_assessment
  ├── professor_confirmed (boolean, nullable)
  └── professor_notes
```

### 8.2 API Architecture

```
/api/assignments
  POST   /create                    → Create assignment + prompt
  GET    /:id                       → Get assignment details
  POST   /:id/generate              → Trigger text generation for all students
  POST   /:id/analyze               → Run automated baseline passes
  GET    /:id/calibration           → Get sample texts for professor review
  POST   /:id/calibration/flag      → Professor flags issue
  POST   /:id/calibration/confirm   → Professor confirms rubric
  POST   /:id/release               → Release assignment to students
  GET    /:id/dashboard             → Instructor dashboard data
  POST   /:id/grades/release        → Release grades to students

/api/texts
  GET    /:id                       → Get generated text (student or instructor)
  GET    /:id/rubric                → Get hidden rubric (instructor only)
  GET    /:id/difficulty            → Get difficulty metadata

/api/audit
  GET    /:text_id/annotations      → Get student's annotations
  POST   /:text_id/annotations      → Create annotation
  PUT    /annotations/:id           → Update annotation
  DELETE /annotations/:id           → Delete annotation
  POST   /annotations/:id/evidence  → Attach evidence

/api/scoring
  POST   /:text_id/score            → Run scoring engine
  GET    /:text_id/score            → Get score breakdown
  PUT    /:text_id/score/override   → Professor override

/api/reports
  GET    /student/:student_id/:text_id  → Student report
  GET    /instructor/:assignment_id     → Instructor dashboard
  GET    /export/:assignment_id         → Research export (anonymized)
```

### 8.3 LLM API Usage Estimate

Per assignment cycle (30 students):

| Operation | Calls | Tokens (approx.) |
|-----------|-------|-------------------|
| Text generation | 30 | 30 × ~1,500 = 45,000 output |
| Baseline passes (6 per text) | 180 | 180 × ~2,000 = 360,000 |
| Professor calibration passes | 30-60 | ~60,000 |
| Beyond-rubric assessment | ~15-30 | ~30,000 |
| Instructor report generation | 30 | ~30,000 |
| **Total per assignment** | **~290-330** | **~525,000 tokens** |

This is manageable for institutional deployment, particularly with batched generation during off-peak hours.

---

## 9. Pedagogical Alignment

### 9.1 Learning Outcomes Addressed

The system directly assesses the following competencies, mapped to established frameworks:

| Competency | Bloom's Level | Assessment Mechanism |
|------------|--------------|---------------------|
| Source verification | Evaluate | Evidence attachment + verification annotations |
| Error detection | Analyze | Error annotations matched against rubric |
| Argumentative assessment | Evaluate | Structural annotations + quality of analysis |
| Disciplinary judgment | Evaluate / Create | Nuance annotations + beyond-rubric findings |
| Alternative generation | Create | Alternative annotations + quality of proposals |
| Metacognitive reflection | Evaluate | Quality and depth of annotation notes |
| AI literacy | Apply / Evaluate | Overall engagement pattern + "accepted" justifications |

### 9.2 Alignment with Existing Frameworks

- **AI Assessment Scale (AIAS)**: The system operates at Level 4-5 (Full AI / AI Exploration) with structured critical engagement
- **PAIRR Model**: The audit workflow parallels the "Review + Reflection" stages, with the hidden rubric replacing peer review as the comparison point
- **APSE Model**: Critical Awareness (recognizing AI limitations), Critical Strategies (verification methods), and Critical Evaluation (quality of analysis) are all directly assessed
- **MLA-CCCC Critical AI Literacy**: Functional literacy (using AI tools for verification), Rhetorical literacy (assessing argument quality), and Critical literacy (questioning outputs) are embedded in the annotation types

### 9.3 Developmental Trajectory

Over a semester, the system can track student growth across these dimensions. Early assignments might show:
- High "accepted" rates with weak justification
- Surface-level error detection (typos, obvious factual errors)
- Minimal beyond-rubric findings

Later assignments should show:
- More nuanced annotation types (disciplinary nuance, alternatives)
- Deeper verification practices (multiple sources, primary sources)
- Increased beyond-rubric findings reflecting growing expertise

This trajectory data is itself a publishable research contribution.

---

## 10. Ethical Considerations & Limitations

### 10.1 Transparency

- Students know the text is AI-generated
- Students know they are graded on audit quality
- Students know the scoring criteria (tiers and categories)
- Students do NOT know the specific hidden rubric items — this is standard assessment practice, equivalent to not sharing an answer key

### 10.2 Fairness Mechanisms

- Difficulty normalization adjusts for text variance
- Professor override prevents algorithmic rigidity
- Beyond-rubric tier prevents ceiling effects
- Coverage scoring accounts for text structure, not just percentage

### 10.3 Known Limitations

- **Automated pass quality depends on LLM accuracy**: If the baseline analysis misidentifies something as an error, students who "find" it get undeserved credit. Professor calibration mitigates but doesn't eliminate this.
- **Semantic matching is imperfect**: Students may describe issues differently than the rubric, leading to false negatives in Tier 1 matching. The multi-signal matching algorithm (location + semantics + type) reduces but doesn't eliminate this.
- **Gaming potential**: Students could use AI to audit the AI-generated text. However, this produces a qualitatively different engagement pattern (high volume, generic language, no evidence of verification process) that the scoring engine can detect and flag.
- **Cultural and linguistic context**: The system needs adaptation for different academic traditions, particularly regarding what counts as "critical engagement" across cultures.

### 10.4 Data & Privacy

- All student data is scoped to the course
- Research exports are anonymized
- AI-generated texts are not stored beyond the assignment cycle unless the student consents
- Professor override logs are retained for academic integrity purposes

---

## 11. Implementation Roadmap

### Phase 1: Minimal Viable Product (Weeks 1-6)

- Student audit workspace (based on existing prototype)
- Manual text generation (professor pastes AI output)
- Basic annotation + evidence + report generation
- No automated baseline — professor creates rubric manually

### Phase 2: Automated Baseline (Weeks 7-12)

- Integrated text generation from prompt
- Automated 6-pass baseline analysis
- Hidden rubric generation
- Basic scoring engine (Tier 1 + Tier 2)

### Phase 3: Professor Calibration (Weeks 13-18)

- Calibration interface
- Professor observation → automated pass conversion
- Difficulty normalization
- Full 3-tier scoring

### Phase 4: Dashboard & Analytics (Weeks 19-24)

- Instructor dashboard with class overview
- Per-student drill-down
- Aggregate analytics
- Research export

### Phase 5: Scale & Research (Ongoing)

- Multi-course deployment
- Cross-disciplinary adaptation
- Longitudinal student development tracking
- Publication of findings

---

## 12. Design System

### 12.1 Design Philosophy

The visual design serves a specific pedagogical function: it must signal **academic seriousness** without creating anxiety, and it must sustain **extended critical reading** without visual fatigue. The aesthetic is best described as *calm scholarly* — the feel of a well-organized research library, not a startup dashboard or a gamified learning app.

**Core design principles:**

1. **No visual aggression.** No pure white (#FFFFFF), no pure black (#000000), no high-saturation accent colors, no sharp shadows. Every surface is slightly warm and muted. The goal is an environment where a student can spend 45 minutes reading and annotating without eye strain.

2. **No decorative elements.** No emoji, no illustration, no gratuitous iconography. Every visual element has a functional purpose. Icons are clean SVG strokes — they communicate annotation type, not personality.

3. **Typography carries the hierarchy.** Rather than using bold colors or heavy borders to distinguish sections, the system relies on font weight, size, and spacing. This mirrors the conventions of academic publishing that students are being trained to engage with.

4. **RTL-native, not RTL-adapted.** The Hebrew version is not a mirror of the English version. It is designed from the ground up for right-to-left reading flow, with border accents, sidebar placement, and reading direction structurally correct — not CSS-flipped.

5. **Annotation colors are the only saturated elements.** The six annotation types use the only high-chroma colors in the interface. This creates an intentional contrast: the document lives in warm neutrals, and the student's critical engagement with it introduces color. The more you audit, the more color appears. This is a subtle visual metaphor for the pedagogical model — the student's thinking is what brings the text to life.

### 12.2 Color System

#### Background & Surface Tokens

| Token | Hex | Usage |
|-------|-----|-------|
| `bg` | `#F3F1EC` | Page background, app shell, inset panels |
| `card` | `#FAFAF6` | Card surfaces, sidebar, top bar, input backgrounds |
| `cardHover` | `#F7F6F1` | Hover states on cards, textarea backgrounds, subtle emphasis |
| `border` | `#DDD9CE` | Primary borders — panels, cards, dividers |
| `borderLight` | `#E8E4DA` | Secondary borders — inner elements, form fields |

**No pure white anywhere.** The lightest surface (`card` at `#FAFAF6`) has a warm yellow-cream undertone. The darkest background (`bg` at `#F3F1EC`) reads as parchment. Together they create depth without contrast fatigue.

#### Text Tokens

| Token | Hex | Usage |
|-------|-----|-------|
| `ink` | `#2C2924` | Primary text — headings, body copy, labels |
| `inkSoft` | `#5E574B` | Secondary text — descriptions, annotation notes, metadata |
| `inkFaint` | `#9E9688` | Tertiary text — timestamps, hints, disabled states, placeholders |

**No pure black for text.** `ink` is a dark warm brown that reads as black at body sizes but doesn't create the harsh contrast of `#000000` against off-white backgrounds. This is a deliberate accessibility-aware choice — sufficient contrast (WCAG AA compliant) without visual strain.

#### Accent Token

| Token | Hex | Usage |
|-------|-----|-------|
| `accent` | `#8B5E3C` | Primary interaction color — buttons, active tab indicators, focus states |
| `accentSoft` | `#8B5E3C18` | Accent backgrounds — badges, notice panels |

The accent is a muted terracotta/cognac — warm, authoritative, distinctly non-tech. It reads as "library" or "leather-bound journal," reinforcing the academic frame. It is used sparingly: primary action buttons, the active tab underline, and notice badges. It never appears on text directly.

#### Annotation Type Colors

These are the only saturated colors in the system. Each must be:
- Distinguishable from all others at a glance
- Readable as a background tint (at 8-20% opacity) behind text
- Accessible as foreground text on off-white backgrounds
- Meaningful in hue association (red = error, green = verified, etc.)

| Type | Token | Hex | Semantic | Soft variant (for backgrounds) |
|------|-------|-----|----------|-------------------------------|
| Error / Inaccuracy | `error` | `#B54D4D` | Muted red — problem, attention | `#B54D4D14` |
| Verified Accurate | `success` | `#4D8B6A` | Sage green — confirmed, safe | `#4D8B6A14` |
| Better Alternative | `warn` | `#A68A2B` | Ochre/gold — consider, compare | `#A68A2B14` |
| Missing Element | `gap` | `#9B6B42` | Warm brown — absent, incomplete | `#9B6B4214` |
| Disciplinary Nuance | `info` | `#4A6F8B` | Slate blue — insight, depth | `#4A6F8B14` |
| Accepted With Reason | `neutral` | `#7A7568` | Warm grey — deliberate, considered | `#7A756814` |

**Highlighted text rendering:** When text is annotated, the annotation color is applied at two levels:
- **Background fill** at 10-20% opacity (`{color}1A` for normal, `{color}30` for active/selected)
- **Bottom border** at full saturation, 2px solid

This creates a subtle highlighting effect that doesn't obscure readability but clearly marks annotated regions. Multiple overlapping annotations are not currently supported — the first annotation on a text range takes precedence.

#### Color Usage Rules

- Never use annotation colors for non-annotation UI elements
- Never use `accent` for annotation-related elements
- Background tints for annotation badges use `{color}14` (8% opacity)
- Background tints for annotation inline highlights use `{color}1A` (10% opacity)
- Border tints for annotation elements use `{color}33` (20% opacity) or `{color}44` (27% opacity)
- The `bg` token is used for inset/recessed areas; `card` for raised/foreground areas
- Buttons: primary uses `accent` bg with off-white text (`#FFF9F4`); secondary uses `card` bg with `ink` text and `border` stroke

### 12.3 Typography

#### Font Stack

```
Primary (all text): 'Heebo', 'Rubik', 'Arial Hebrew', system-ui, sans-serif
```

**Heebo** is the primary font — a clean Hebrew-first sans-serif designed by Oded Ezer, available on Google Fonts. It has excellent weight range (100-900), strong Latin character support, and was designed for screen readability. It works for both display and body text, eliminating the need for a separate display font.

**Rubik** is the first fallback — another Hebrew-optimized sans-serif with similar proportions. **Arial Hebrew** is the system fallback for macOS/iOS. **system-ui** catches everything else.

For English-language deployment, the font stack becomes:
```
'Heebo', Georgia, 'Times New Roman', serif  (display/headings)
'Heebo', system-ui, -apple-system, 'Segoe UI', Helvetica, sans-serif  (body)
```

The English version may use a serif display font (Georgia) for headings to create an editorial/academic feel, while keeping the body in a clean sans-serif. The Hebrew version uses Heebo throughout because Hebrew serifs reduce readability at body sizes on screen.

**Note for implementation:** When deploying as a web application, Heebo should be loaded via `@import` in CSS or `<link>` in the HTML head — NOT inside React component JSX. For the prototype/artifact environment, fall back to system fonts since external font loading may be blocked.

#### Type Scale

| Element | Size | Weight | Line Height | Token | Usage |
|---------|------|--------|-------------|-------|-------|
| Page title | 38px | 500 | 1.2 | `display` | Landing page headline only |
| Section heading | 28px | 500 | 1.3 | `display` | Report title, major section heads |
| Card heading | 16px | 500-600 | 1.4 | `body` | Sidebar section titles, panel headers |
| Body text (document) | 15.5px | 400 | 2.0 | `display` | The AI-generated text being audited |
| Body text (UI) | 13-14px | 400-500 | 1.6-1.7 | `body` | Interface labels, descriptions, form fields |
| Small text | 12-12.5px | 400-600 | 1.5-1.7 | `body` | Annotation notes, evidence items, metadata |
| Micro text | 10-11px | 500-600 | 1.4 | `body` | Badges, tags, timestamps, legend items |

**Key decision — document line height of 2.0:** The AI-generated text that students audit uses a generous line height (2.0). This is intentionally more spacious than typical body copy because: (a) students need to be able to select precise text ranges for annotation, (b) highlighted annotations need visual breathing room, and (c) extended critical reading benefits from spacious line heights. This mirrors the line spacing conventions of draft manuscripts submitted for editorial review.

#### Hebrew-Specific Typography Rules

- No italic for emphasis in Hebrew body text — use font weight (500 vs 400) instead, as Hebrew italic is less readable than Latin italic
- Quoted text uses guillemets (« ») rather than quotation marks (" "), following Hebrew academic convention
- Numbers and Latin text within Hebrew flow render LTR within the RTL context automatically (Unicode Bidi algorithm) — no manual direction overrides needed
- Parenthetical citations remain in Latin characters and render naturally within RTL flow

### 12.4 Iconography

All icons are inline SVG, rendered as React components. No emoji, no icon fonts, no external icon libraries.

#### Icon Design Specifications

- **Stroke width:** 1.6-2.2px depending on icon size (thinner strokes at larger sizes)
- **Stroke cap:** Round (`strokeLinecap="round"`)
- **Stroke join:** Round (`strokeLinejoin="round"`)
- **Fill:** Generally `none` (stroke-only); small fills only for emphasis elements (e.g., center dot of target icon)
- **Default size:** 16px for inline icons, 18-20px for navigation icons, 24px for featured/hero icons
- **Color:** Always passed as a prop, never hardcoded. Icons inherit their context color.

#### Icon Set

| Name | Usage | Description |
|------|-------|-------------|
| `search` | App logo, empty states | Magnifying glass — circle + diagonal line |
| `x` | Error annotation type | X mark — two crossing diagonal lines |
| `check` | Verified annotation type | Checkmark — angled polyline |
| `swap` | Alternative annotation type | Bidirectional arrows — two arrows with arrowheads pointing opposite directions |
| `triangle` | Gap/missing annotation type | Open triangle outline |
| `target` | Nuance annotation type | Concentric circles (3 rings) with filled center dot |
| `dot` | Accepted annotation type | Circle with semi-transparent filled inner dot |
| `chat` | AI conversation evidence | Speech bubble |
| `source` | Source/verification evidence | Open book |
| `note` | Process note evidence | Document with lines |
| `file` | Document reference | Document with folded corner |
| `arrow` | Back navigation | Left-pointing arrow (flips to right in RTL) |
| `plus` | Add evidence button | Plus sign |
| `close` | Dismiss/close buttons | X mark (lighter than error icon) |

#### Icon Color Rules

- Annotation type icons use their annotation type color
- Evidence type icons use `inkSoft`
- Navigation icons use `ink` or `inkFaint`
- Interactive icons use `inkFaint` at rest, `ink` on hover
- Icons in primary buttons use off-white (`#FFF9F4`)
- Icons in badges use their badge's text color

### 12.5 Layout Architecture

#### Workspace Layout (Primary View)

```
┌──────────────────────────────────────────────────────────────────┐
│  TOP BAR: App logo + title | Stats + "Generate Report" button   │
├──────────────────────────────────────────────────────────────────┤
│  AI AUTHORSHIP BANNER: "Written by AI" badge + context message  │
├──────────────────────────────────────────────────────────────────┤
│  SELECTION TOOLBAR (conditional): "Tag as:" + type buttons      │
├──────────────────┬───────────────────────────────────────────────┤
│                  │                                               │
│    SIDEBAR       │         DOCUMENT PANEL                       │
│    (370px)       │         (flex: 1)                            │
│                  │                                               │
│  ┌────────────┐  │   Scrollable AI-generated text               │
│  │ Tab bar:   │  │   with inline annotation highlights          │
│  │ Audit |    │  │                                               │
│  │ Prompt |   │  │   Padding: 28px 44px                         │
│  │ Guide      │  │                                               │
│  ├────────────┤  │                                               │
│  │            │  │                                               │
│  │ Tab content│  │                                               │
│  │ (scrolls)  │  │                                               │
│  │            │  │                                               │
│  └────────────┘  │                                               │
│                  │                                               │
├──────────────────┴───────────────────────────────────────────────┤
│  LEGEND BAR: Annotation type color chips + labels               │
└──────────────────────────────────────────────────────────────────┘
```

**RTL layout note:** In the Hebrew version, the sidebar is on the RIGHT (reading-start side). The document panel is on the left. In the English version, the sidebar is on the LEFT. This is not a CSS `direction: rtl` flip — the component order is structurally different.

#### Landing Page Layout

Centered single-column layout, max-width 680px. Vertical stack:
1. Icon (search icon in accent-tinted circle)
2. Title (38px)
3. Subtitle (15px, `inkSoft`)
4. Explanatory text (13px, `inkFaint`)
5. Main card containing: prompt display, title input, text preview, "Begin Audit" button
6. Four-step process indicators (grid, 4 columns)

#### Report Layout

Single-column layout, max-width 820px, centered. Vertical stack:
1. Header (title + back button)
2. Four stat cards (grid, 4 columns)
3. Annotation breakdown (bar chart)
4. Scoring indicators (2-column grid)
5. Detailed annotations by type (grouped sections)
6. Instructor assessment panel (accent-bordered card)

#### Instructor Dashboard Layout (Phase 4)

Full-width with left navigation sidebar (200px). Main content area with:
1. Class overview cards (stat grid)
2. Score distribution visualization
3. Most caught / most missed issues lists
4. Beyond-rubric findings requiring review
5. Expandable per-student cards

### 12.6 Component Specifications

#### Buttons

**Primary button:**
```
Background: accent (#8B5E3C)
Text color: #FFF9F4 (warm off-white, not pure white)
Border: none
Border radius: 7px
Padding: 9px 18px
Font: body, 13px, weight 500
Hover: darken background 8%
Disabled: opacity 0.45, cursor not-allowed
```

**Secondary button:**
```
Background: card (#FAFAF6)
Text color: ink (#2C2924)
Border: 1px solid border (#DDD9CE)
Border radius: 7px
Padding: 9px 18px
Font: body, 13px, weight 500
Hover: background cardHover (#F7F6F1)
```

**Small button (tags, prompts):**
```
Border radius: 10px (more pill-shaped)
Padding: 4px 12px
Font size: 12px
Otherwise follows primary/secondary patterns
```

**Annotation type tag buttons (selection toolbar):**
```
Background: {annotationType.color}14
Border: 1px solid {annotationType.color}44
Border radius: 10px
Text color: {annotationType.color}
Font: body, 11.5px, weight 500
Display: flex, align-items center, gap 4px (icon + label)
```

#### Cards

**Annotation card (sidebar):**
```
Background: card
Border: 1px solid border
Border-right (RTL) or border-left (LTR): 4px solid {annotationType.color}
Border radius: 8px
Padding: 14px
Margin-bottom: 10px
```

**Stat card (report):**
```
Background: card
Border: 1px solid border
Border radius: 10px
Padding: 14px 16px
Text align: center
Value: display font, 28px, weight 700, ink
Label: body font, 11px, inkFaint
```

**Notice/info card:**
```
Background: accentSoft (#8B5E3C18)
Border: 1px solid accent at 25% opacity
Border radius: 8px
Padding: 12px
Font: body, 12px, line-height 1.7
```

#### Form Elements

**Text input:**
```
Background: cardHover (#F7F6F1)
Border: 1px solid border (#DDD9CE)
Border radius: 7px
Padding: 9px 12px
Font: body, 13px
Direction: rtl (Hebrew) / ltr (English)
Focus: border-color accent
```

**Textarea:**
```
Same as text input, plus:
Min-height: 55px
Resize: vertical
Line-height: 1.7
```

**Tabs:**
```
Container: flex, border-bottom 1px solid border
Tab button: flex 1, padding 10px
Active: background bg, border-bottom 2px solid accent, font-weight 600, color ink
Inactive: background card, border-bottom 2px solid transparent, font-weight 400, color inkFaint
```

#### Badges

**Status badge (e.g., "READ-ONLY", "AI-AUTHORED"):**
```
Font: body, 10px, weight 600
Padding: 2px 8px
Border radius: 6px
Color + background determined by context:
  - Read-only: accent text, accentSoft background
  - AI-authored: info text, infoSoft background
```

**Annotation type badge:**
```
Display: inline-flex, align-items center, gap 5px
Background: {color}14
Border: 1px solid {color}33
Border radius: 8px
Padding: 3px 10px
Font: body, 11px, weight 600
Color: {annotationType.color}
Contains: SVG icon (13px) + label text
```

#### Text Highlighting (Annotated Document)

```
Annotated span:
  Background: {color}1A (normal) / {color}30 (active)
  Border-bottom: 2px solid {color}
  Padding: 1px
  Border-radius: 2px
  Cursor: pointer
  Transition: all 0.2s

Active/selected annotation:
  Outline: 2px solid {color}
  Outline-offset: 1px
```

#### Evidence Item

```
Background: cardHover (in sidebar) / bg (in report)
Border: 1px solid borderLight
Border-radius: 5px
Padding: 6px 8px
Margin-bottom: 4px
Font: body, 11.5px
Layout: flex, align-items flex-start, gap 5-6px
Icon: evidence type SVG (13px, inkSoft)
Label: bold, inkSoft
Content: normal weight, inkSoft
```

#### Guided Prompt Chips

```
Background: cardHover (#F7F6F1)
Border: 1px solid {annotationType.color} at 15% opacity
Border-radius: 8px
Padding: 3px 10px
Font: body, 10.5px, inkSoft
Cursor: pointer
Hover: background shifts toward annotation color tint
```

### 12.7 Spacing System

The system uses a base-4 spacing scale:

| Token | Value | Usage |
|-------|-------|-------|
| `xs` | 4px | Gaps between inline elements, icon margins |
| `sm` | 6-8px | Inner padding (badges, chips), small gaps |
| `md` | 10-12px | Component padding, sidebar content padding, list gaps |
| `lg` | 14-16px | Card padding, section spacing |
| `xl` | 20-28px | Panel padding, section margins |
| `xxl` | 32-44px | Document panel padding, major section spacing |

**Document panel padding:** 28px vertical, 44px horizontal — generous, allowing the text to breathe and giving annotated passages room to display their highlights without crowding the panel edges.

**Sidebar padding:** 12px — tighter than the document panel because the sidebar contains denser, smaller UI elements.

### 12.8 Motion & Transitions

Minimal, functional transitions only. No decorative animations, no entrance animations, no loading spinners beyond a simple text change ("Generating..." on buttons).

| Element | Property | Duration | Easing |
|---------|----------|----------|--------|
| Buttons | background, opacity | 150ms | ease |
| Annotation highlights | background-color, outline | 200ms | ease |
| Tab switching | border-bottom color | 150ms | ease |
| Selection toolbar | appearance (conditional render) | Instant (no animation) |

**No animation on:** page transitions (landing → workspace → report), sidebar tab content switches, annotation card creation/deletion.

**Rationale:** This is an academic tool for sustained critical work. Animations that draw attention to the interface draw attention away from the text. The interface should feel like it responds immediately and then gets out of the way.

### 12.9 Responsive Behavior

**Desktop (>1024px):** Full layout as specified — sidebar + document panel side by side.

**Tablet (768-1024px):** Sidebar collapses to an overlay/drawer that slides in from the reading-start side (right for Hebrew, left for English). Document panel takes full width. Floating action button in bottom corner opens sidebar.

**Mobile (<768px):** Single-column layout. Top bar remains fixed. Document and sidebar become vertically stacked tabs (document tab / audit tab). Selection toolbar becomes a bottom sheet. Report view remains single-column (already responsive).

**Landing page:** Responsive by default (single centered column). The four-step process indicators stack to 2×2 grid on mobile.

### 12.10 Accessibility

- All colors meet WCAG AA contrast requirements against their intended backgrounds
- All interactive elements are keyboard-navigable (tab order follows visual order)
- SVG icons include appropriate `aria-label` attributes when used as standalone interactive elements
- Annotation type colors are distinguishable in common forms of color blindness (tested against deuteranopia and protanopia simulations) — the combination of color + icon + label provides redundant encoding
- Focus states use the `accent` color outline (2px solid, 2px offset)
- Text selection (for annotation creation) works with keyboard (Shift+Arrow) as well as mouse
- All form fields have associated labels (explicit or via `aria-label`)
- The document viewer supports screen readers — annotated text spans include `title` attributes describing the annotation type

### 12.11 Dark Mode (Future)

Not implemented in v1. When added, the following transformations apply:

- `bg` → `#1E1D1A` (warm dark)
- `card` → `#2A2824` (slightly lighter warm dark)
- `ink` → `#E8E4DA` (warm light, not pure white)
- Annotation colors retain their hue but shift to higher saturation against dark backgrounds
- `accent` remains unchanged (terracotta reads well on dark and light)

### 12.12 File Naming & Asset Conventions

- All icons are defined as inline React SVG components, not external files
- Color tokens are defined as a single JavaScript constant object (`C = { ... }`)
- Font stacks are defined as a single JavaScript constant object (`Font = { display: ..., body: ... }`)
- Component files use PascalCase: `AuditWorkspace.jsx`, `AnnotationCard.jsx`, `ReportView.jsx`
- No CSS files — all styling is inline via React `style` objects
- Hebrew and English versions share the same component architecture; language-specific content (labels, prompts, sample text) is externalized into locale objects
